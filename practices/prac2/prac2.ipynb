{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aA14kYCYSwo"
      },
      "source": [
        "## Динамическое программирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roQ4hAstjVce"
      },
      "source": [
        "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
        "$$\n",
        "V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
        "$$\n",
        "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
        "$$\n",
        "Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
        "$$\n",
        "$$\n",
        "V_{(i+1)}(s) = \\max_a Q_i(s,a)\n",
        "$$\n",
        "\n",
        "Зададим напрямую модель MDP с картинки:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l6LwgNvgYXIP"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !wget https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OpKyGJEJYYDn"
      },
      "outputs": [],
      "source": [
        "transition_probs = {\n",
        "  's0':{\n",
        "    'a0': {'s0': 0.5, 's2': 0.5},\n",
        "    'a1': {'s2': 1}\n",
        "  },\n",
        "  's1':{\n",
        "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "    'a1': {'s1': 0.95, 's2': 0.05}\n",
        "  },\n",
        "  's2':{\n",
        "    'a0': {'s0': 0.4, 's2': 0.6},\n",
        "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
        "  }\n",
        "}\n",
        "rewards = {\n",
        "  's1': {'a0': {'s0': +5}},\n",
        "  's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "\n",
        "from mdp import MDP\n",
        "import numpy as np\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSC6KXuYcsh"
      },
      "source": [
        "Теперь мы можем использовать это MDP, как и любое другое gym окружение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PzLyFJ4iYfro"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial state = s0\n",
            "next_state =s2, reward = 0.0, done = False\n"
          ]
        }
      ],
      "source": [
        "state = mdp.reset()\n",
        "print('initial state =', state)\n",
        "next_state, reward, done, info = mdp.step('a1')\n",
        "print(f'next_state ={next_state}, reward = {reward}, done = {done}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgRdVPJlYjZ4"
      },
      "source": [
        ":Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4zK1xXedYn21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_states = ('s0', 's1', 's2')\n",
            "possible_actions('s1') =  ('a0', 'a1')\n",
            "next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
            "reward('s1', 'a0', 's0') =  5\n",
            "transition_prob('s1', 'a0', 's0') =  0.7\n"
          ]
        }
      ],
      "source": [
        "print(\"all_states =\", mdp.get_all_states())\n",
        "print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
        "print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
        "print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n",
        "print(\"transition_prob('s1', 'a0', 's0') = \",\n",
        "      mdp.get_transition_prob('s1', 'a0', 's0'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oe_RzZtYq11"
      },
      "source": [
        "### Задание 1\n",
        "\n",
        "Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n",
        "\n",
        "---\n",
        "\n",
        "`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n",
        "\n",
        "`2.` For $i=0, 1, 2, \\dots$\n",
        "\n",
        "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n",
        "\n",
        "---\n",
        "\n",
        "Вначале вычисляем оценку состояния-действия:\n",
        "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aA0DQccjody"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Qt0o0MokYv0F"
      },
      "outputs": [],
      "source": [
        "def get_action_value(\n",
        "    mdp, state_values, state, action, gamma\n",
        "):\n",
        "    \"\"\" Вычисляем Q(s,a) по формуле выше \"\"\"\n",
        "    # вычислеяем оценку состояния\n",
        "    # Q =\n",
        "    ####### Здесь ваш код ########\n",
        "    return sum([mdp.get_transition_prob(state, action, next_state) \n",
        "                * (mdp.get_reward(state, action, next_state) + gamma * state_values[next_state]) \n",
        "                for next_state in mdp.get_next_states(state, action)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x06WscSIYysp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6900000000000002\n",
            "3.95\n"
          ]
        }
      ],
      "source": [
        "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
        "print(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9))\n",
        "print(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9))\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87q6GhsMY19h"
      },
      "source": [
        "Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n",
        "\n",
        "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O3QFuoVj1iZ"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hFqCuRaBY5J_"
      },
      "outputs": [],
      "source": [
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return 0\n",
        "    # V =\n",
        "    ####### Здесь ваш код ########\n",
        "    return max([get_action_value(mdp, state_values, state, action, gamma) for action in mdp.get_possible_actions(state)])\n",
        "    ##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lPUyRzQOY8PP"
      },
      "outputs": [],
      "source": [
        "test_Vs_copy = dict(test_Vs)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
        "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
        "   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
        "assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od-SBiPKY_Q3"
      },
      "source": [
        "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-0-PlvmkF7P"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uUwb5JCDZDD4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter    0 | diff: 3.50000 | V(start): 0.000 \n",
            "iter    1 | diff: 0.64500 | V(start): 0.000 \n",
            "iter    2 | diff: 0.58050 | V(start): 0.581 \n",
            "iter    3 | diff: 0.43582 | V(start): 0.866 \n",
            "iter    4 | diff: 0.30634 | V(start): 1.145 \n",
            "iter    5 | diff: 0.27571 | V(start): 1.421 \n",
            "iter    6 | diff: 0.24347 | V(start): 1.655 \n",
            "iter    7 | diff: 0.21419 | V(start): 1.868 \n",
            "iter    8 | diff: 0.19277 | V(start): 2.061 \n",
            "iter    9 | diff: 0.17327 | V(start): 2.233 \n",
            "iter   10 | diff: 0.15569 | V(start): 2.389 \n",
            "iter   11 | diff: 0.14012 | V(start): 2.529 \n",
            "iter   12 | diff: 0.12610 | V(start): 2.655 \n",
            "iter   13 | diff: 0.11348 | V(start): 2.769 \n",
            "iter   14 | diff: 0.10213 | V(start): 2.871 \n",
            "iter   15 | diff: 0.09192 | V(start): 2.963 \n",
            "iter   16 | diff: 0.08272 | V(start): 3.045 \n",
            "iter   17 | diff: 0.07445 | V(start): 3.120 \n",
            "iter   18 | diff: 0.06701 | V(start): 3.187 \n",
            "iter   19 | diff: 0.06031 | V(start): 3.247 \n",
            "iter   20 | diff: 0.05428 | V(start): 3.301 \n",
            "iter   21 | diff: 0.04885 | V(start): 3.350 \n",
            "iter   22 | diff: 0.04396 | V(start): 3.394 \n",
            "iter   23 | diff: 0.03957 | V(start): 3.434 \n",
            "iter   24 | diff: 0.03561 | V(start): 3.469 \n",
            "iter   25 | diff: 0.03205 | V(start): 3.502 \n",
            "iter   26 | diff: 0.02884 | V(start): 3.530 \n",
            "iter   27 | diff: 0.02596 | V(start): 3.556 \n",
            "iter   28 | diff: 0.02336 | V(start): 3.580 \n",
            "iter   29 | diff: 0.02103 | V(start): 3.601 \n",
            "iter   30 | diff: 0.01892 | V(start): 3.620 \n",
            "iter   31 | diff: 0.01703 | V(start): 3.637 \n",
            "iter   32 | diff: 0.01533 | V(start): 3.652 \n",
            "iter   33 | diff: 0.01380 | V(start): 3.666 \n",
            "iter   34 | diff: 0.01242 | V(start): 3.678 \n",
            "iter   35 | diff: 0.01117 | V(start): 3.689 \n",
            "iter   36 | diff: 0.01006 | V(start): 3.699 \n",
            "iter   37 | diff: 0.00905 | V(start): 3.708 \n",
            "iter   38 | diff: 0.00815 | V(start): 3.717 \n",
            "iter   39 | diff: 0.00733 | V(start): 3.724 \n",
            "iter   40 | diff: 0.00660 | V(start): 3.731 \n",
            "iter   41 | diff: 0.00594 | V(start): 3.736 \n",
            "iter   42 | diff: 0.00534 | V(start): 3.742 \n",
            "iter   43 | diff: 0.00481 | V(start): 3.747 \n",
            "iter   44 | diff: 0.00433 | V(start): 3.751 \n",
            "iter   45 | diff: 0.00390 | V(start): 3.755 \n",
            "iter   46 | diff: 0.00351 | V(start): 3.758 \n",
            "iter   47 | diff: 0.00316 | V(start): 3.762 \n",
            "iter   48 | diff: 0.00284 | V(start): 3.764 \n",
            "iter   49 | diff: 0.00256 | V(start): 3.767 \n",
            "iter   50 | diff: 0.00230 | V(start): 3.769 \n",
            "iter   51 | diff: 0.00207 | V(start): 3.771 \n",
            "iter   52 | diff: 0.00186 | V(start): 3.773 \n",
            "iter   53 | diff: 0.00168 | V(start): 3.775 \n",
            "iter   54 | diff: 0.00151 | V(start): 3.776 \n",
            "iter   55 | diff: 0.00136 | V(start): 3.778 \n",
            "iter   56 | diff: 0.00122 | V(start): 3.779 \n",
            "iter   57 | diff: 0.00110 | V(start): 3.780 \n",
            "iter   58 | diff: 0.00099 | V(start): 3.781 \n",
            "Принято! Алгоритм сходится!\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(\n",
        "    mdp, state_values=None,\n",
        "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5\n",
        "):\n",
        "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
        "    # инициализируем V(s)\n",
        "    state_values = state_values or \\\n",
        "    {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "    new_state_values = state_values.copy()\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        # Вычисляем новые полезности состояний,\n",
        "        # используя функции, определенные выше.\n",
        "        # Должен получиться словарь {s: new_V(s)}\n",
        "        # new_state_values =\n",
        "        ####### Здесь ваш код ########\n",
        "        new_state_values = {state: get_new_state_value(mdp, state_values, state, gamma) for state in mdp.get_all_states()}\n",
        "        ##############################\n",
        "\n",
        "        assert isinstance(new_state_values, dict)\n",
        "\n",
        "        # Считаем разницу\n",
        "        diff = max(\n",
        "            abs(new_state_values[s] - state_values[s])\n",
        "            for s in mdp.get_all_states()\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"iter {i:4} | diff: {diff:6.5f} \"\n",
        "            f\"| V(start): {new_state_values[mdp._initial_state]:.3f} \"\n",
        "        )\n",
        "\n",
        "        state_values = new_state_values.copy()\n",
        "        if diff < min_difference:\n",
        "            print(\"Принято! Алгоритм сходится!\")\n",
        "            break\n",
        "\n",
        "    return state_values\n",
        "\n",
        "state_values = value_iteration(\n",
        "    mdp, num_iter = 100, min_difference = 0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZKomkPSrZGlZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final state values: {'s0': 3.7810348735476405, 's1': 7.29400642386723, 's2': 4.202140275227049}\n"
          ]
        }
      ],
      "source": [
        "print(\"Final state values:\", state_values)\n",
        "\n",
        "assert abs(state_values['s0'] - 3.781) < 0.01\n",
        "assert abs(state_values['s1'] - 7.294) < 0.01\n",
        "assert abs(state_values['s2'] - 4.202) < 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gz5JxncZJoX"
      },
      "source": [
        "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
        "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml9AWeYrkNgf"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7gd4m26TZOn3"
      },
      "outputs": [],
      "source": [
        "def get_optimal_action(\n",
        "    mdp, state_values, state, gamma=0.9\n",
        "):\n",
        "    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n",
        "    if mdp.is_terminal(state): return None\n",
        "\n",
        "    actions = mdp.get_possible_actions(state)\n",
        "    # выбираем лучшее действие\n",
        "    # i =\n",
        "    ####### Здесь ваш код ########\n",
        "    i = np.argmax([get_action_value(mdp, state_values, state, action, gamma) for action in mdp.get_possible_actions(state)])\n",
        "    ##############################\n",
        "\n",
        "    return actions[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yv3MRqaQZSzs"
      },
      "outputs": [],
      "source": [
        "assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n",
        "assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n",
        "assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n",
        "\n",
        "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
        "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
        "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
        "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V1KMZyhbZVVX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average reward:  0.4598\n"
          ]
        }
      ],
      "source": [
        "# Проверим среднее вознаграждение агента\n",
        "\n",
        "s = mdp.reset()\n",
        "rewards = []\n",
        "for _ in range(10000):\n",
        "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
        "    rewards.append(r)\n",
        "\n",
        "print(\"average reward: \", np.mean(rewards))\n",
        "\n",
        "assert(0.40 < np.mean(rewards) < 0.55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkokwmulZYYn"
      },
      "source": [
        "### Задание 2\n",
        "\n",
        "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E4V34IMzZbeH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*FFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "iter    0 | diff: 1.00000 | V(start): 0.000 \n",
            "iter    1 | diff: 0.90000 | V(start): 0.000 \n",
            "iter    2 | diff: 0.81000 | V(start): 0.000 \n",
            "iter    3 | diff: 0.72900 | V(start): 0.000 \n",
            "iter    4 | diff: 0.65610 | V(start): 0.000 \n",
            "iter    5 | diff: 0.59049 | V(start): 0.590 \n",
            "iter    6 | diff: 0.00000 | V(start): 0.590 \n",
            "Принято! Алгоритм сходится!\n"
          ]
        }
      ],
      "source": [
        "from mdp import FrozenLakeEnv\n",
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "\n",
        "mdp.render()\n",
        "state_values = value_iteration(mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNPtPQo2ZdpV"
      },
      "source": [
        "Визуализируем нашу стратегию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aQP4HnjNZg4C"
      },
      "outputs": [],
      "source": [
        "def draw_policy(mdp, state_values, gamma=0.9):\n",
        "    \"\"\"функция визуализации стратегии\"\"\"\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    h, w = mdp.desc.shape\n",
        "    states = sorted(mdp.get_all_states())\n",
        "    V = np.array([state_values[s] for s in states])\n",
        "    Pi = {\n",
        "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
        "        for s in states\n",
        "    }\n",
        "    plt.imshow(\n",
        "        V.reshape(w, h),\n",
        "        cmap='gray', interpolation='none',\n",
        "        clim=(0, 1)\n",
        "    )\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(h) - .5)\n",
        "    ax.set_yticks(np.arange(w) - .5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    Y, X = np.mgrid[0:4, 0:4]\n",
        "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
        "            'right': (1, 0), 'up': (-1, 0)}\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
        "                     color='g', size=12,\n",
        "                     verticalalignment='center',\n",
        "                     horizontalalignment='center',\n",
        "                     fontweight='bold')\n",
        "            a = Pi[y, x]\n",
        "            if a is None: continue\n",
        "            u, v = a2uv[a]\n",
        "            plt.arrow(x, y, u * .3, -v * .3,\n",
        "                      color='m', head_width=0.1,\n",
        "                      head_length=0.1)\n",
        "    plt.grid(color='b', lw=2, ls='-')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UJ2zkkx2Zlec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after iteration 29\n",
            "iter    0 | diff: 0.00000 | V(start): 0.198 \n",
            "Принято! Алгоритм сходится!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
        "state_values = {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(30):\n",
        "    clear_output(True)\n",
        "    print(\"after iteration %i\"%i)\n",
        "    state_values = value_iteration(mdp,\n",
        "                            state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "    sleep(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nltqOBDfZoFG"
      },
      "source": [
        "Посмотрим на оптимальную стратегию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CKJ1oJapZq77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*FFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "S*FFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SF*FFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFF*FFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFF*FFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFFF*FF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFFFF*F\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFF*F\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFF*F\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFH*F\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHF*\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFF*\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFF*\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFF*F\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFF*\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFH*\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFH*\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF*\n",
            "\n"
          ]
        }
      ],
      "source": [
        "s = mdp.reset()\n",
        "mdp.render()\n",
        "for t in range(100):\n",
        "    A = get_optimal_action(mdp, state_values, s, 0.9)\n",
        "    print(A, end='\\n\\n')\n",
        "    s, r, done, _ = mdp.step(A)\n",
        "    mdp.render()\n",
        "    if done:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksq-NonlZtHM"
      },
      "source": [
        "Тестируем на более сложном варианте окружения:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6g4fbKkkZza"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yOBqWNBfZv6v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter    0 | diff: 0.80000 | V(start): 0.000 \n",
            "iter    1 | diff: 0.57600 | V(start): 0.000 \n",
            "iter    2 | diff: 0.41472 | V(start): 0.000 \n",
            "iter    3 | diff: 0.29860 | V(start): 0.000 \n",
            "iter    4 | diff: 0.24186 | V(start): 0.000 \n",
            "iter    5 | diff: 0.19349 | V(start): 0.000 \n",
            "iter    6 | diff: 0.15325 | V(start): 0.000 \n",
            "iter    7 | diff: 0.12288 | V(start): 0.000 \n",
            "iter    8 | diff: 0.09930 | V(start): 0.000 \n",
            "iter    9 | diff: 0.08037 | V(start): 0.000 \n",
            "iter   10 | diff: 0.06426 | V(start): 0.000 \n",
            "iter   11 | diff: 0.05129 | V(start): 0.000 \n",
            "iter   12 | diff: 0.04330 | V(start): 0.000 \n",
            "iter   13 | diff: 0.03802 | V(start): 0.033 \n",
            "iter   14 | diff: 0.03332 | V(start): 0.058 \n",
            "iter   15 | diff: 0.02910 | V(start): 0.087 \n",
            "iter   16 | diff: 0.01855 | V(start): 0.106 \n",
            "iter   17 | diff: 0.01403 | V(start): 0.120 \n",
            "iter   18 | diff: 0.00810 | V(start): 0.128 \n",
            "iter   19 | diff: 0.00555 | V(start): 0.133 \n",
            "iter   20 | diff: 0.00321 | V(start): 0.137 \n",
            "iter   21 | diff: 0.00247 | V(start): 0.138 \n",
            "iter   22 | diff: 0.00147 | V(start): 0.139 \n",
            "iter   23 | diff: 0.00104 | V(start): 0.140 \n",
            "iter   24 | diff: 0.00058 | V(start): 0.140 \n",
            "iter   25 | diff: 0.00036 | V(start): 0.141 \n",
            "iter   26 | diff: 0.00024 | V(start): 0.141 \n",
            "iter   27 | diff: 0.00018 | V(start): 0.141 \n",
            "iter   28 | diff: 0.00012 | V(start): 0.141 \n",
            "iter   29 | diff: 0.00007 | V(start): 0.141 \n",
            "iter   30 | diff: 0.00004 | V(start): 0.141 \n",
            "iter   31 | diff: 0.00003 | V(start): 0.141 \n",
            "iter   32 | diff: 0.00001 | V(start): 0.141 \n",
            "iter   33 | diff: 0.00001 | V(start): 0.141 \n",
            "Принято! Алгоритм сходится!\n",
            "Cреднее вознаграждение: 0.757\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        # выполняем оптимальное действие в окружении\n",
        "        ####### Здесь ваш код ########\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, 0.9)\n",
        "            )\n",
        "        ##############################\n",
        "\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
        "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
        "print(\"Принято!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOEo-OheZyYP"
      },
      "source": [
        "### Задание 3\n",
        "\n",
        "Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n",
        "\n",
        "---\n",
        "Initialize $\\pi_0$   `// случайно`\n",
        "\n",
        "For $n=0, 1, 2, \\dots$\n",
        "- Считаем функцию $V^{\\pi_{n}}$\n",
        "- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n",
        "- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
        "---\n",
        "\n",
        "PI включает в себя оценку полезности состояния, как внутренний шаг."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diaeh1f7Z010"
      },
      "source": [
        "Вначале оценим полезности, используя текущую стратегию:\n",
        "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
        "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pFDjkE2kfsY"
      },
      "source": [
        "### 3 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RpV4Yw8Z3bi"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import solve\n",
        "\n",
        "def compute_vpi(mdp, policy, gamma):\n",
        "    \"\"\"\n",
        "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
        "    :param policy: словарь состояние->действие {s : a}\n",
        "    :returns: словарь {state : V^pi(state)}\n",
        "    \"\"\"\n",
        "    states = mdp.get_all_states()\n",
        "    state_indices = {state: idx for idx, state in enumerate(states)}\n",
        "    A = np.zeros((len(states), len(states)))\n",
        "    b = []\n",
        "    for i, s in enumerate(states):\n",
        "        A[i][i] = 1\n",
        "        if mdp.is_terminal(s):\n",
        "            b.append(0)\n",
        "        else: \n",
        "            b.append(np.sum([mdp.get_transition_prob(s, policy[s], s_) * \n",
        "                         mdp.get_reward(s, policy[s], s_) \n",
        "                            for s_ in mdp.get_next_states(s, policy[s])]))\n",
        "            for s_ in mdp.get_next_states(s, policy[s]):\n",
        "                A[i][state_indices[s_]] += - gamma * mdp.get_transition_prob(s, policy[s], s_)\n",
        "\n",
        "    A = np.array(A)\n",
        "    b = np.array(b)\n",
        "    values = solve(A, b)\n",
        "\n",
        "    state_values = {\n",
        "        states[i] : values[i]\n",
        "        for i in range(len(states))\n",
        "    }\n",
        "    return state_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qeb79E20Z6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'s0': 3.78994861511468, 's1': 7.3029201654342675, 's2': 4.211054016794089}\n"
          ]
        }
      ],
      "source": [
        "transition_probs = {\n",
        "    's0': {\n",
        "        'a0': {'s0': 0.5, 's2': 0.5},\n",
        "        'a1': {'s2': 1}\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "        'a1': {'s1': 0.95, 's2': 0.05}\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': {'s0': 0.4, 's1': 0.6},\n",
        "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "rewards = {\n",
        "    's1': {'a0': {'s0': +5}},\n",
        "    's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "test_policy = {\n",
        "    s: np.random.choice(mdp.get_possible_actions(s))\n",
        "    for s in mdp.get_all_states()}\n",
        "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
        "\n",
        "print(new_vpi)\n",
        "assert type(new_vpi) is dict, \\\n",
        "    \"функция compute_vpi должна возвращать словарь \\\n",
        "    {состояние s : V^pi(s) }\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2YNXpxZ9BT"
      },
      "source": [
        "Теперь обновляем стратегию на основе новых значений полезностей:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsYlHPblkrSI"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TGCHMeaUZ_Qj"
      },
      "outputs": [],
      "source": [
        "def compute_new_policy(mdp, vpi, gamma):\n",
        "    \"\"\"\n",
        "    Рассчитываем новую стратегию\n",
        "    :param vpi: словарь {state : V^pi(state) }\n",
        "    :returns: словарь {state : оптимальное действие}\n",
        "    \"\"\"\n",
        "    # Q = {}\n",
        "    # for state in mdp.get_all_states():\n",
        "    #     Q[state] = {}\n",
        "    #     for a in mdp.get_possible_actions(state):\n",
        "    #         values = []\n",
        "    #         for next_state in mdp.get_next_states(state, a):\n",
        "    #             r = mdp.get_reward(state, a, next_state)\n",
        "    #             p = mdp.get_transition_prob(\n",
        "    #                 state, a, next_state\n",
        "    #             )\n",
        "    #             # values.append(...)\n",
        "    #             ####### Здесь ваш код ########\n",
        "    #             raise NotImplementedError\n",
        "    #             ##############################\n",
        "\n",
        "    #         Q[state][a] = sum(values)\n",
        "\n",
        "    # policy = {}\n",
        "    # for state in mdp.get_all_states():\n",
        "    #     actions = mdp.get_possible_actions(state)\n",
        "    #     if actions:\n",
        "    #         # выбираем оптимальное действие в state\n",
        "    #         # policy[state] = ...\n",
        "    #         ####### Здесь ваш код ########\n",
        "    #         raise NotImplementedError\n",
        "    #         ##############################\n",
        "\n",
        "    return {s: get_optimal_action(mdp, vpi, s) for s in mdp.get_all_states()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1b1OXlg9aBsy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'s0': 'a1', 's1': 'a0', 's2': 'a0'}\n"
          ]
        }
      ],
      "source": [
        "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
        "\n",
        "print(new_policy)\n",
        "\n",
        "assert type(new_policy) is dict, \\\n",
        "\"функция compute_new_policy должна возвращать словарь \\\n",
        "{состояние s: оптимальное действие}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15mJglOZaEmI"
      },
      "source": [
        "Собираем все в единый цикл:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIrgcIqKkxD2"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2LcLHHhIaHAZ"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(\n",
        "    mdp, policy=None, gamma = 0.9,\n",
        "    num_iter = 1000, min_difference = 1e-5\n",
        "):\n",
        "    \"\"\"\n",
        "    Запускаем цикл итерации по стратегиям\n",
        "    Если стратегия не определена, задаем случайную\n",
        "    \"\"\"\n",
        "    for i in range(num_iter):\n",
        "        if not policy:\n",
        "            policy = {}\n",
        "            for s in mdp.get_all_states():\n",
        "                if mdp.get_possible_actions(s):\n",
        "                    policy[s] = (\n",
        "                        np.random.choice(mdp.get_possible_actions(s))\n",
        "                    )\n",
        "        state_values = compute_vpi(mdp, policy, gamma)\n",
        "        new_policy = compute_new_policy(mdp, state_values, gamma)\n",
        "        if all(policy[s] == new_policy[s] for s in policy):\n",
        "            break  \n",
        "        policy = new_policy\n",
        "\n",
        "    return state_values, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddfLTSfgaJjU"
      },
      "source": [
        "Тестируем на FrozenLake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4hLv3X0OaKmg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average reward:  0.888\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
        "state_values, policy = policy_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(policy[s])\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
        "print(\"Принято!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3-12-11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
